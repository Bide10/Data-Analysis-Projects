{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8f3cbf",
   "metadata": {},
   "source": [
    "# <center> Customer Churn Prediction using Artificial Neural Neworks </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6886faf",
   "metadata": {},
   "source": [
    "### In this ANN project I am going to perform basic neural network operations starting from the very 1st step of dataset importing to final optimization. The following are the steps:\n",
    "- Importing dataset\n",
    "- Performing basic preprocessing operations such as Label Encoding and One Hot Encoding\n",
    "- Standardizing the data using standard scaler\n",
    "- creating the ANN model with appropriate parameters\n",
    "- Training the ANN model on our dataset\n",
    "- Evaluating the performance of the model with predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa8687",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181f1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6580a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset \n",
    "\n",
    "df = pd.read_csv('churn_modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4262f52-9b3b-471b-b658-8bd5d965dc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafcefd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>5632</td>\n",
       "      <td>15782758</td>\n",
       "      <td>Ozerova</td>\n",
       "      <td>632</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>147650.68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199674.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>6252</td>\n",
       "      <td>15614520</td>\n",
       "      <td>Smith</td>\n",
       "      <td>682</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>148580.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35179.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId  Surname  CreditScore Geography  Gender  Age  \\\n",
       "5631       5632    15782758  Ozerova          632    France    Male   40   \n",
       "6251       6252    15614520    Smith          682    France  Female   37   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "5631       5  147650.68              1          1               1   \n",
       "6251       8  148580.12              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "5631        199674.83       0  \n",
       "6251         35179.18       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2045c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the dataset into dependent and independent variables by slicing. and while at it, removing those variables which has\n",
    "# no impact on the decision of the model, like, RowNumber, CustomerID, Surname. \n",
    "x = df.iloc[:,3:-1].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554db5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc640",
   "metadata": {},
   "source": [
    "#### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1454e9-bc24-4c89-9ddd-080c0ada650c",
   "metadata": {},
   "source": [
    "- we have categorical columns which are not ideal for a NN to train on, so we need to use dummy variables.\n",
    "- we have 2 kinds of categorical variables here. Binary and Nominal. i.e., Gender is Binary - Male & Female\n",
    "- geographical data which is the Country has no order in it so thats nominal variable.\n",
    "- for Binary categorical variable we use Label Encoder\n",
    "- for Nominal categorical variable we use One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc81ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "x[:,2] = le.fit_transform(x[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58caac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding:\n",
    "\n",
    "# Column Transformer - is a function of sklearn.compose module which allows the transformation of different columns \n",
    "# separately and the features generated by the transformations are then concatenated into a single feature space\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers = [('encoder',OneHotEncoder(),[1])],remainder = 'passthrough')\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fd525",
   "metadata": {},
   "source": [
    "#### Performing data splitting and scaling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac6099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2, random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecaa6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing feature scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4498c",
   "metadata": {},
   "source": [
    "### Part 2: Building ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436ea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model\n",
    "\n",
    "ann = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb948545",
   "metadata": {},
   "source": [
    "#### Building the input layer and 1st hidden layer\n",
    "\n",
    "number of units = average of number of inputs and output <br>\n",
    "in our case ; Units = (11+1)/2 = 6, <br>\n",
    "this is basically the number of nodes in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ed1cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(Dense(units=6,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2e84f",
   "metadata": {},
   "source": [
    "#### Building 2nd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f25cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(Dense(units=6,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a6b23",
   "metadata": {},
   "source": [
    "#### Building output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ab7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73517d",
   "metadata": {},
   "source": [
    "### Part 3 : Training the ANN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0550f016-11ad-4487-83ab-fd571c84da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compiling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731c5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd424686-1b85-4144-8930-9c2d36a774ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f766e850-c0e2-4cf1-b2ea-d0b1b2792ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 2s 4ms/step - loss: 0.6578 - accuracy: 0.6275\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4864 - accuracy: 0.7969\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4533 - accuracy: 0.7993\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4371 - accuracy: 0.8049\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4286 - accuracy: 0.8091\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4239 - accuracy: 0.8105\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4203 - accuracy: 0.8135\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4171 - accuracy: 0.8154\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4131 - accuracy: 0.8175\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4084 - accuracy: 0.8199\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4032 - accuracy: 0.8234\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3964 - accuracy: 0.8273\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3892 - accuracy: 0.8309\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3821 - accuracy: 0.8349\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3755 - accuracy: 0.8410\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3695 - accuracy: 0.8435\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3646 - accuracy: 0.8481\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3603 - accuracy: 0.8496\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3563 - accuracy: 0.8515\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3529 - accuracy: 0.8529\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3506 - accuracy: 0.8551\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3491 - accuracy: 0.8543\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3475 - accuracy: 0.8559\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3461 - accuracy: 0.8550\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3451 - accuracy: 0.8581\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3443 - accuracy: 0.8576\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3439 - accuracy: 0.8573\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3430 - accuracy: 0.8584\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3427 - accuracy: 0.8593\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3419 - accuracy: 0.8564\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3415 - accuracy: 0.8596\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3409 - accuracy: 0.8613\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3408 - accuracy: 0.8590\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3404 - accuracy: 0.8593\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3401 - accuracy: 0.8599\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3393 - accuracy: 0.8603\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3387 - accuracy: 0.8594\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3383 - accuracy: 0.8606\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3382 - accuracy: 0.8615\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8625\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3373 - accuracy: 0.8614\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3371 - accuracy: 0.8633\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3366 - accuracy: 0.8633\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8631\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3359 - accuracy: 0.8644\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3356 - accuracy: 0.8638\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3355 - accuracy: 0.8623\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3349 - accuracy: 0.8639\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3348 - accuracy: 0.8646\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3343 - accuracy: 0.8640\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3341 - accuracy: 0.8651\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3341 - accuracy: 0.8636\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3339 - accuracy: 0.8653\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3335 - accuracy: 0.8649\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3334 - accuracy: 0.8646\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3334 - accuracy: 0.8643\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3334 - accuracy: 0.8651\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.8641\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.8634\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3327 - accuracy: 0.8644\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8651\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3327 - accuracy: 0.8641\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3332 - accuracy: 0.8649\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3327 - accuracy: 0.8644\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3321 - accuracy: 0.8650\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3322 - accuracy: 0.8645\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3322 - accuracy: 0.8644\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3320 - accuracy: 0.8654\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3318 - accuracy: 0.8645\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3319 - accuracy: 0.8651\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3318 - accuracy: 0.8655\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3317 - accuracy: 0.8660\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8659\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3319 - accuracy: 0.8653\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3315 - accuracy: 0.8649\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3315 - accuracy: 0.8656\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3313 - accuracy: 0.8646\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3311 - accuracy: 0.8651\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3312 - accuracy: 0.8648\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3311 - accuracy: 0.8655\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3307 - accuracy: 0.8651\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8641\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3310 - accuracy: 0.8660\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3304 - accuracy: 0.8640\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3308 - accuracy: 0.8656\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3307 - accuracy: 0.8655\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3309 - accuracy: 0.8656\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3306 - accuracy: 0.8643\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3304 - accuracy: 0.8646\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3302 - accuracy: 0.8658\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3305 - accuracy: 0.8654\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8648\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3305 - accuracy: 0.8658\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3304 - accuracy: 0.8658\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3302 - accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3302 - accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3304 - accuracy: 0.8656\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3296 - accuracy: 0.8661\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3302 - accuracy: 0.8651\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3300 - accuracy: 0.8651\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    ann.fit(x_train,y_train,batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eec6a9-c1fd-40be-acdb-98c3475ceafb",
   "metadata": {},
   "source": [
    "### Part 4: Predicting Making the predictions and evaluating the model 😱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3f7d7-2c69-4e1d-a4e6-cbb698be951d",
   "metadata": {},
   "source": [
    "Now using this ANN model to predict if the customer with the following informations will leave the bank or not: \n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: \\$ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: \\$ 50000\n",
    "\n",
    "So, should we say goodbye to that customer? 😰😰😰😰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f3b3bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "# now that the model has been trained we need to predict new data using our model.\n",
    "# passing all the values in an orderly fashion as per the dataset to predict churning \n",
    "\n",
    "print(ann.predict(scaler.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f20d12-c2f9-43f6-a2b4-c141c341a141",
   "metadata": {},
   "source": [
    "Therefore, our ANN model predicts that this customer stays in the bank! 😌😌😌😌\n",
    "\n",
    "**Important note:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa41f0-d3ec-4046-bcbf-725e31317ded",
   "metadata": {},
   "source": [
    "#### Making a Confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd2dd21-220d-4746-99cb-f6eaebc9505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8ab48-ecad-420a-8e6a-186be13ec8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f58b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1514   81]\n",
      " [ 198  207]]\n",
      "Accuracy : 0.8605\n",
      "Precision : 0.8843457943925234\n",
      "f1 score : 0.9156335046870275\n",
      "recall : 0.9492163009404389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy :' ,accuracy_score(y_test, y_pred))\n",
    "print('Precision :' ,precision_score(y_test, y_pred, pos_label=0))\n",
    "print('f1 score :' ,f1_score(y_test, y_pred, pos_label=0))\n",
    "print('recall :' ,recall_score(y_test, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ceff9-9689-427d-86a5-b1f00ce0580d",
   "metadata": {},
   "source": [
    "True Positives = 1514 <br>\n",
    "False Positive = 81 <br>\n",
    "True Negative = 198 <br>\n",
    "False Negative = 207 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e60de-aff6-457b-ad3c-f621010f5c04",
   "metadata": {},
   "source": [
    "## Conclusion, Business inferences and further scope from this project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb373c-8730-4af7-b5c7-5a70cf1e462d",
   "metadata": {},
   "source": [
    "**Accuracy** of 86.05% signifies that the model can correctly predict churn or no churn for 86.05% of the cases. <br>\n",
    "**Precision** of 88.43% implies that out of all the customers predicted to churn, 88.43% of them did churn, which is a crucial model reliability factor for data driven business decision making. <br>\n",
    "**Spicificity** calculated as True Negative /(True Negative + False Positive) = 70.96% which basically means that the model correctly identifies customers who are not likely to churn.  <br>\n",
    "**Recall** The model captures 94.92% of the actual churners correctly which is an important decision making factor for business. <br>\n",
    "**f1 score** This score is denoting that how well is the model balancing **Precision** & **Recall**. And a score of 91.56 % signifies the model performs well in both correctly identifying churners and minimizing false alarms.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ce01c-95f2-47c0-985d-4a8f15211953",
   "metadata": {},
   "source": [
    "### Business Value of the model: <br>**  \r\n",
    "\r\n",
    "The customer churn prediction model, built using Artificial Neural Networks (ANNs), demonstrates a strong predictive capability with an **accuracy of 86.05%**, indicating reliable identification of customers at risk of leaving. Additionally:  \r\n",
    "\r\n",
    "- **High Recall (94.92%)** ensures the model identifies the vast majority of customers likely to churn, allowing the business to proactively intervene and retain valuable customers.  \r\n",
    "- **Precision (88.43%)** signifies that most of the identified churn-risk customers truly need attention, ensuring marketing or retention efforts are not wasted.  \r\n",
    "- The **F1-score (91.56%)**, balancing precision and recall, highlights the model’s overall robustness in managing churn prediction effectively.  \r\n",
    "\r\n",
    "By implementing this model:  \r\n",
    "1. **Improve Customer Retention**: Identifying and addressing churn risks before customers leave helps reduce attrition rates, ensuring sustained revenue streams and long-term business growth.  \r\n",
    "2. **Optimize Resource Allocation**: The high precision minimizes unnecessary expenditure by focusing retention strategies on genuinely at-risk customers.  \r\n",
    "3. **Enhance Customer Experience**: Proactive engagement with at-risk customers can strengthen loyalty and foster stronger relationships.  \r\n",
    "4. **Boost Profitability**: Retaining existing customers is more cost-effective than acquiring new ones, directly improving the bottom line.  \r\n",
    "\r\n",
    "This model can act as a critical decision-support tool for retention teams, empowering them to make data-driven interventions that yield tangible ess outcomes.\n",
    "\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d3108-703b-401b-994e-ddf86a1dbf9c",
   "metadata": {},
   "source": [
    "### Further scope of the project: <br>\n",
    "\n",
    "1. Data Refinement:\n",
    "    - Feature Engineering: Add more relevant features that may influence churn (e.g., customer interaction history, satisfaction scores, product usage patterns).\n",
    "          Use domain knowledge to create meaningful derived metrics (e.g., lifetime value, inactivity duration).\n",
    "    - Data Balancing: Address any class imbalance by techniques such as SMOTE (Synthetic Minority Oversampling Technique) or undersampling to ensure the model              doesn't favor the majority class.\n",
    "    - Address Outliers and Missing Data: Clean the dataset further to handle anomalies that may skew predictions.\n",
    "  \n",
    "2. Model Optimization:\n",
    "    - Hyperparameter Tuning: Optimize the ANN’s architecture, including the number of layers, neurons, learning rate, and activation functions using techniques like        grid search or Bayesian optimization.\n",
    "    - Regularization: Apply L1/L2 regularization or dropout layers to prevent overfitting.\n",
    "    - Advanced Architectures: Experiment with recurrent neural networks (RNNs) or attention mechanisms if temporal data (e.g., sequential transactions) is available.\n",
    "  \n",
    "3. Improve Training Data Quality:\n",
    "    - Augment Data Sources: Integrate additional data from customer surveys, support tickets, or social media sentiments to capture latent factors influencing churn.\n",
    "    - Behavioral Segmentation: Segment customers into cohorts based on demographics, usage, or profitability, and build tailored models for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb482a4-071e-4993-bce9-7a989bff96fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
