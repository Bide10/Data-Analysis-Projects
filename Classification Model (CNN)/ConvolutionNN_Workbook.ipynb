{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d00af06-989e-4c32-b1f0-e972fdcc5c3b",
   "metadata": {},
   "source": [
    "# Cat vs. Dog Classification Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48f44f-1688-4ea8-bb69-7688b502859b",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd21225-81af-416b-9cff-de63f99b84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14339ba-7aa5-4ba3-bd66-d47f8682d600",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727ba91-476b-472e-a04a-eb828d1fea66",
   "metadata": {},
   "source": [
    "#### Preprocessing Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d216ce98-895f-417e-b365-396c8e4a6b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# using the image data generator api from keras - this contains some preprocessing formats and we can use that to perform required image preprocessing.\n",
    "\n",
    "image_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "training_set = image_datagen.flow_from_directory(\n",
    "    'C:/Users/Bidee/OneDrive/Desktop/Data Science/Deep_Learning_A_Z/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/Section 40 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
    "    target_size = (150,150),\n",
    "    batch_size= 32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f2356-0af6-4a8d-97bd-5fa9ff578219",
   "metadata": {},
   "source": [
    "#### Preprocessing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6ba57c-0436-41a0-8e1d-42df18a75f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# we dont need to apply any transformations into our test datasets as those are like new images\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'C:/Users/Bidee/OneDrive/Desktop/Data Science/Deep_Learning_A_Z/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/Section 40 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
    "    target_size = (150,150),\n",
    "    batch_size= 32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1e5a0-af3d-4f34-9dd8-fafd5a2b1ac2",
   "metadata": {},
   "source": [
    "### Part 2: Build The CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9769935-0ba6-4e30-a27c-a7a59573677f",
   "metadata": {},
   "source": [
    "#### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106c1a67-5440-4d55-a2ad-bca4f933a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d0283-edde-472f-b150-df2b1c1a2db7",
   "metadata": {},
   "source": [
    "#### Step 1: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b92ba6-3caa-4e77-ae2c-86e387de1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the 1st layer of convolution, here we are adding all the matrices i.e., filters, kernels etc. input shape is required for the 1st input layers of CNN.\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32,kernel_size = 2, activation = 'leaky_relu', input_shape=[150,150,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ca872-ad60-4634-9c17-c42de59e0bab",
   "metadata": {},
   "source": [
    "#### Step 2: Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8dfc264-36b5-473a-b49d-e09025ab0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are taking a max pooling size with a 2x2 matrix and stride will be 2. \n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd92867-69f8-46fa-93b7-70036c01ce2d",
   "metadata": {},
   "source": [
    "#### Adding a second convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c10beef-9e73-4f4b-867a-3cab7d950aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while adding subsequent layers, thus forming deep neural network, we don't need input shapes anymore \n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32,kernel_size = 2, activation = 'leaky_relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4842209-f171-4956-b921-9e5ca83fa32d",
   "metadata": {},
   "source": [
    "#### Step 3: Flatteing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f9201f-5bc9-4318-9353-fc625a22bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are flattening all the convolutions into a 1D array which will be fed to the fully connected neural network.\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017c0af-b389-40d7-b4d7-2926806fbd86",
   "metadata": {},
   "source": [
    "#### Step 4: Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac7813a-bf2b-49e6-8cea-84405a736462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our good ol ANN. The only thing changed is the number of hidden layers. Since dealing with CNN is complicated its better to have a deep ANN for better learning.\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'leaky_relu' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42ab14-fa71-4ce0-9c4a-b312f4ab2088",
   "metadata": {},
   "source": [
    "#### Step 5: Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c353a892-f952-41c2-b495-c5528bfaf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer will have 1 output and sigmoid activation as usual.\n",
    "# note: If we're doing multi class classification, we would be using softmax activation function\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42b0b2-f19a-4048-b03c-bc488c368444",
   "metadata": {},
   "source": [
    "### Part 3: Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51984a49-9823-4e8e-a6f2-1daeaade39a4",
   "metadata": {},
   "source": [
    "#### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a352a35-b367-43d0-b381-36753cad98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526f6c4-b677-422e-8df6-6399dcecefe9",
   "metadata": {},
   "source": [
    "#### Training the CNN on Training set and testing it on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9971ed-c1f2-4cd6-9a19-5ea6fea76265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 54s 211ms/step - loss: 0.6911 - accuracy: 0.6108 - val_loss: 0.6260 - val_accuracy: 0.6695\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.6058 - accuracy: 0.6626 - val_loss: 0.5892 - val_accuracy: 0.7045\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 0.5548 - accuracy: 0.7124 - val_loss: 0.5604 - val_accuracy: 0.7045\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 46s 185ms/step - loss: 0.5257 - accuracy: 0.7350 - val_loss: 0.5298 - val_accuracy: 0.7270\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.5054 - accuracy: 0.7538 - val_loss: 0.5238 - val_accuracy: 0.7630\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.4837 - accuracy: 0.7665 - val_loss: 0.5038 - val_accuracy: 0.7485\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.4667 - accuracy: 0.7781 - val_loss: 0.4974 - val_accuracy: 0.7585\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.4447 - accuracy: 0.7928 - val_loss: 0.4887 - val_accuracy: 0.7730\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.4338 - accuracy: 0.7931 - val_loss: 0.4793 - val_accuracy: 0.7800\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 0.4261 - accuracy: 0.8028 - val_loss: 0.4555 - val_accuracy: 0.7880\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.4095 - accuracy: 0.8125 - val_loss: 0.4804 - val_accuracy: 0.7845\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 47s 190ms/step - loss: 0.4065 - accuracy: 0.8156 - val_loss: 0.5423 - val_accuracy: 0.7640\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.3935 - accuracy: 0.8170 - val_loss: 0.5055 - val_accuracy: 0.7800\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.3872 - accuracy: 0.8255 - val_loss: 0.4784 - val_accuracy: 0.7910\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 49s 196ms/step - loss: 0.3736 - accuracy: 0.8329 - val_loss: 0.5122 - val_accuracy: 0.7825\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.3652 - accuracy: 0.8386 - val_loss: 0.4785 - val_accuracy: 0.7890\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.3590 - accuracy: 0.8419 - val_loss: 0.4889 - val_accuracy: 0.7880\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.3462 - accuracy: 0.8454 - val_loss: 0.5176 - val_accuracy: 0.7790\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.3353 - accuracy: 0.8563 - val_loss: 0.4933 - val_accuracy: 0.8020\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 0.3300 - accuracy: 0.8546 - val_loss: 0.5246 - val_accuracy: 0.7930\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.3275 - accuracy: 0.8563 - val_loss: 0.5198 - val_accuracy: 0.7870\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.3204 - accuracy: 0.8568 - val_loss: 0.5198 - val_accuracy: 0.7875\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 50s 200ms/step - loss: 0.3071 - accuracy: 0.8671 - val_loss: 0.4921 - val_accuracy: 0.7985\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.2966 - accuracy: 0.8723 - val_loss: 0.5049 - val_accuracy: 0.7995\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.2907 - accuracy: 0.8729 - val_loss: 0.5285 - val_accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "# unlike normal training and testings, here we're training and testing it on the validation data at the same time.\n",
    "with tf.device('/GPU:0'):\n",
    "    cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec813bfe-7d2d-41a8-b222-6b5eab3a5824",
   "metadata": {},
   "source": [
    "### Part 4: Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eccc95e-4500-44b3-80a3-5cefdad70f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 186ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "img_path = 'C:/Users/Bidee/OneDrive/Desktop/Data Science/Deep_Learning_A_Z/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/Section 40 - Convolutional Neural Networks (CNN)/dataset/single_prediction/cat_or_dog_1.jpg'\n",
    "test_image = load_img(img_path,target_size = (150,150))\n",
    "#converting the target image from jpg to a numpy array which is the desired dtype.\n",
    "test_image = img_to_array(test_image)\n",
    "# we are expanding the dimensions cuz previously we specified the batch size. This basically means images are expected by the model in batch format.\n",
    "# so, the model will take , batch 1: 32 images, batch 2: 32 images and so on to train and test. Hence it will be expecting the same when it comes to prediction.\n",
    "# taking the number of batches as a separate dimension we can say one batch has a dimension of: 32,150,150.\n",
    "# so, when feeding new images for classifications, we want a fake dimension containing a value of 1 as our added dimension to 150, 150 as we want to predict \n",
    "# a single image's class, the net dimension would become 1, 150, 150.\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69732b14-81ff-463f-8f91-64ed418a2634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "# now we need to do some decodings manually for final prediction.\n",
    "# we can use the following code to see what encodings for the classes has been done:\n",
    "training_set.class_indices\n",
    "\n",
    "# the following is our decodings: \n",
    "if result[0][0] >0.5 :\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b5149-eed8-47c6-92a0-bcfdfaffbfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
